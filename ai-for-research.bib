@online{battleUnreasonableEffectivenessEccentric2024a,
  title = {The {{Unreasonable Effectiveness}} of {{Eccentric Automatic Prompts}}},
  author = {Battle, Rick and Gollapudi, Teja},
  date = {2024-02-20},
  eprint = {2402.10949},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2402.10949},
  url = {http://arxiv.org/abs/2402.10949},
  urldate = {2024-10-21},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus computation time, of experimenting with hand-tuning prompts for large black-box models, we then compared the performance of the best "positive thinking" prompt against the output of systematic prompt optimization. We show that employing an automated prompt optimizer emerges as the most effective method for enhancing performance, even when working with smaller open-source models. Additionally, our findings reveal that the highest-scoring, automatically-optimized prompt exhibits a degree of peculiarity far beyond expectations.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-10-21T15:18:53.911Z},
  file = {/Users/andrew/Zotero/storage/AFC8U9GW/Battle and Gollapudi - 2024 - The Unreasonable Effectiveness of Eccentric Automatic Prompts.pdf;/Users/andrew/Zotero/storage/FCQ4ZR6Z/2402.html}
}

@online{cuiEffectsGenerativeAI2024a,
  type = {SSRN Scholarly Paper},
  title = {The {{Effects}} of {{Generative AI}} on {{High Skilled Work}}: {{Evidence}} from {{Three Field Experiments}} with {{Software Developers}}},
  shorttitle = {The {{Effects}} of {{Generative AI}} on {{High Skilled Work}}},
  author = {Cui, Zheyuan (Kevin) and Demirer, Mert and Jaffe, Sonia and Musolff, Leon and Peng, Sida and Salz, Tobias},
  date = {2024-09-03},
  number = {4945566},
  eprint = {4945566},
  eprinttype = {Social Science Research Network},
  location = {Rochester, NY},
  doi = {10.2139/ssrn.4945566},
  url = {https://papers.ssrn.com/abstract=4945566},
  urldate = {2024-11-09},
  abstract = {This study evaluates the impact of generative AI on software developer productivity by analyzing data from three randomized controlled trials conducted at Microsoft, Accenture, and an anonymous Fortune 100 electronics manufacturing company. These field experiments, which were run by the companies as part of their ordinary course of business, provided a randomly selected subset of developers with access to GitHub Copilot, an AI-based coding assistant that suggests intelligent code completions. Though each separate experiment is noisy, combined across all three experiments and 4,867 software developers, our analysis reveals a 26.08\% increase (SE: 10.3\%) in the number of completed tasks among developers using the AI tool. Notably, less experienced developers showed higher adoption rates and greater productivity gains.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Leon Musolff,Mert Demirer,Sida Peng,Sonia Jaffe,SSRN,The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers,Tobias Salz,Zheyuan (Kevin) Cui},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-09T21:35:44.843Z},
  file = {/Users/andrew/Zotero/storage/HLE26E8E/Cui et al. - 2024 - The Effects of Generative AI on High Skilled Work Evidence from Three Field Experiments with Softwa.pdf}
}

@article{damischKeepYourFingers2010,
  title = {Keep {{Your Fingers Crossed}}!: {{How Superstition Improves Performance}}},
  shorttitle = {Keep {{Your Fingers Crossed}}!},
  author = {Damisch, Lysann and Stoberock, Barbara and Mussweiler, Thomas},
  date = {2010-07},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {21},
  number = {7},
  pages = {1014--1020},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797610372631},
  url = {https://journals.sagepub.com/doi/10.1177/0956797610372631},
  urldate = {2024-11-10},
  abstract = {Superstitions are typically seen as inconsequential creations of irrational minds. Nevertheless, many people rely on superstitious thoughts and practices in their daily routines in order to gain good luck. To date, little is known about the consequences and potential benefits of such superstitions. The present research closes this gap by demonstrating performance benefits of superstitions and identifying their underlying psychological mechanisms.Specifically,Experiments 1 through 4 show that activating good-luck-related superstitions via a common saying or action (e.g., “break a leg,” keeping one’s fingers crossed) or a lucky charm improves subsequent performance in golfing, motor dexterity, memory, and anagram games. Furthermore, Experiments 3 and 4 demonstrate that these performance benefits are produced by changes in perceived self-efficacy. Activating a superstition boosts participants’ confidence in mastering upcoming tasks, which in turn improves performance. Finally, Experiment 4 shows that increased task persistence constitutes one means by which self-efficacy, enhanced by superstition, improves performance.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-10T13:18:44.255Z},
  file = {/Users/andrew/Zotero/storage/6J7976CL/Damisch et al. - 2010 - Keep Your Fingers Crossed! How Superstition Improves Performance.pdf}
}

@article{dellacquaNavigatingJaggedTechnological2023,
  title = {Navigating the {{Jagged Technological Frontier}}: {{Field Experimental Evidence}} of the {{Effects}} of {{AI}} on {{Knowledge Worker Productivity}} and {{Quality}}},
  shorttitle = {Navigating the {{Jagged Technological Frontier}}},
  author = {Dell'Acqua, Fabrizio and McFowland, Edward and Mollick, Ethan R. and Lifshitz-Assaf, Hila and Kellogg, Katherine and Rajendran, Saran and Krayer, Lisa and Candelon, François and Lakhani, Karim R.},
  date = {2023},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4573321},
  url = {https://www.ssrn.com/abstract=4573321},
  urldate = {2024-11-09},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-09T22:57:23.795Z},
  file = {/Users/andrew/Zotero/storage/DIJN2GJ2/Dell'Acqua et al. - 2023 - Navigating the Jagged Technological Frontier Field Experimental Evidence of the Effects of AI on Kn.pdf}
}

@article{fleckensteinTeachersSpotAI2024a,
  title = {Do Teachers Spot {{AI}}? {{Evaluating}} the Detectability of {{AI-generated}} Texts among Student Essays},
  shorttitle = {Do Teachers Spot {{AI}}?},
  author = {Fleckenstein, Johanna and Meyer, Jennifer and Jansen, Thorben and Keller, Stefan D. and Köller, Olaf and Möller, Jens},
  date = {2024-06},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {6},
  pages = {100209},
  issn = {2666920X},
  doi = {10.1016/j.caeai.2024.100209},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666920X24000109},
  urldate = {2024-05-27},
  abstract = {The potential application of generative artificial intelligence (AI) in schools and universities poses great chal­ lenges, especially for the assessment of students’ texts. Previous research has shown that people generally have difficulty distinguishing AI-generated from human-written texts; however, the ability of teachers to identify an AI-generated text among student essays has not yet been investigated. Here we show in two experimental studies that novice (N = 89) and experienced teachers (N = 200) could not identify texts generated by ChatGPT among student-written texts. However, there are some indications that more experienced teachers made more differ­ entiated and more accurate judgments. Furthermore, both groups were overconfident in their judgments. Effects of real and assumed source on quality assessment were heterogeneous. Our findings demonstrate that with relatively little prompting, current AI can generate texts that are not detectable for teachers, which poses a challenge to schools and universities in grading student essays. Our study provides empirical evidence for the current debate regarding exam strategies in schools and universities in light of the latest technological developments.},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/VV3JVRVV/Fleckenstein et al. - 2024 - Do teachers spot AI Evaluating the detectability .pdf}
}

@article{gelmanProblemsPValuesArea,
  title = {The {{Problems With P-Values}} Are Not {{Just With P-Values}}},
  author = {Gelman, Andrew},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-09T22:12:40.640Z},
  file = {/Users/andrew/Zotero/storage/92EKCRSP/Gelman - The Problems With P-Values are not Just With P-Values.pdf}
}

@online{hadanGreatAIWitch2024,
  title = {The {{Great AI Witch Hunt}}: {{Reviewers Perception}} and ({{Mis}}){{Conception}} of {{Generative AI}} in {{Research Writing}}},
  shorttitle = {The {{Great AI Witch Hunt}}},
  author = {Hadan, Hilda and Wang, Derrick and Mogavi, Reza Hadi and Tu, Joseph and Zhang-Kennedy, Leah and Nacke, Lennart E.},
  date = {2024-06-27},
  eprint = {2407.12015},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2407.12015},
  url = {http://arxiv.org/abs/2407.12015},
  urldate = {2024-11-13},
  abstract = {Generative AI (GenAI) use in research writing is growing fast. However, it is unclear how peer reviewers recognize or misjudge AI-augmented manuscripts. To investigate the impact of AI-augmented writing on peer reviews, we conducted a snippet-based online survey with 17 peer reviewers from top-tier HCI conferences. Our findings indicate that while AI-augmented writing improves readability, language diversity, and informativeness, it often lacks research details and reflective insights from authors. Reviewers consistently struggled to distinguish between human and AI-augmented writing but their judgements remained consistent. They noted the loss of a "human touch" and subjective expressions in AI-augmented writing. Based on our findings, we advocate for reviewer guidelines that promote impartial evaluations of submissions, regardless of any personal biases towards GenAI. The quality of the research itself should remain a priority in reviews, regardless of any preconceived notions about the tools used to create it. We emphasize that researchers must maintain their authorship and control over the writing process, even when using GenAI's assistance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-13T21:54:44.159Z},
  file = {/Users/andrew/Zotero/storage/954A3BDE/Hadan et al. - 2024 - The Great AI Witch Hunt Reviewers Perception and (Mis)Conception of Generative AI in Research Writi.pdf;/Users/andrew/Zotero/storage/MJK9336W/2407.html}
}

@article{ioannidisWhyMostPublished2005a,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  date = {2005-08-30},
  journaltitle = {PLoS Medicine},
  shortjournal = {PLoS Med},
  volume = {2},
  number = {8},
  pages = {e124},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  url = {https://dx.plos.org/10.1371/journal.pmed.0020124},
  urldate = {2024-11-10},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-10T13:53:47.649Z},
  file = {/Users/andrew/Zotero/storage/ADXR7YYP/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf}
}

@article{kambhampatiCanLargeLanguage2024,
  title = {Can {{Large Language Models Reason}} and {{Plan}}?},
  author = {Kambhampati, Subbarao},
  date = {2024-04},
  journaltitle = {Annals of the New York Academy of Sciences},
  shortjournal = {Annals of the New York Academy of Sciences},
  volume = {1534},
  number = {1},
  eprint = {2403.04121},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {15--18},
  issn = {0077-8923, 1749-6632},
  doi = {10.1111/nyas.15125},
  url = {http://arxiv.org/abs/2403.04121},
  urldate = {2024-08-10},
  abstract = {While humans sometimes do show the capability of correcting their own erroneous guesses with self-critiquing, there seems to be no basis for that assumption in the case of LLMs.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/Y7PTQDES/Kambhampati - 2024 - Can Large Language Models Reason and Plan.pdf;/Users/andrew/Zotero/storage/KLI9M3T7/2403.html}
}

@online{kambhampatiLLMsCantPlan2024,
  title = {{{LLMs Can}}'t {{Plan}}, {{But Can Help Planning}} in {{LLM-Modulo Frameworks}}},
  author = {Kambhampati, Subbarao and Valmeekam, Karthik and Guan, Lin and Verma, Mudit and Stechly, Kaya and Bhambri, Siddhant and Saldyt, Lucas and Murthy, Anil},
  date = {2024-06-11},
  eprint = {2402.01817},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.01817},
  url = {http://arxiv.org/abs/2402.01817},
  urldate = {2024-08-14},
  abstract = {There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators. We present a vision of \{\textbackslash bf LLM-Modulo Frameworks\} that combine the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime. We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs. We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, and allows extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.},
  pubstate = {prepublished},
  keywords = {/unread,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/andrew/Zotero/storage/KDLJGI2B/Kambhampati et al. - 2024 - LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks.pdf;/Users/andrew/Zotero/storage/GMZSTD5K/2402.html}
}

@online{liangCanLargeLanguage2023,
  title = {Can Large Language Models Provide Useful Feedback on Research Papers? {{A}} Large-Scale Empirical Analysis},
  shorttitle = {Can Large Language Models Provide Useful Feedback on Research Papers?},
  author = {Liang, Weixin and Zhang, Yuhui and Cao, Hancheng and Wang, Binglu and Ding, Daisy and Yang, Xinyu and Vodrahalli, Kailas and He, Siyu and Smith, Daniel and Yin, Yian and McFarland, Daniel and Zou, James},
  date = {2023-10-03},
  eprint = {2310.01783},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.01783},
  urldate = {2024-11-11},
  abstract = {Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4’s feedback through two large-scale studies. We first quantitatively compared GPT-4’s generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85\% for Nature journals, 39.23\% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58\% for Nature journals, 35.25\% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers (i.e., rejected ICLR papers; average overlap 43.80\%). We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4\%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4\% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations. For example, GPT-4 tends to focus on certain aspects of scientific feedback (e.g., ‘add experiments on more datasets’), and often struggles to provide in-depth critique of method design. Together our results suggest that LLM and human feedback can complement each other. While human expert review is and should continue to be the foundation of rigorous scientific process, LLM feedback could benefit researchers, especially when timely expert feedback is not available and in earlier stages of manuscript preparation before peer-review.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-11T00:25:21.447Z},
  file = {/Users/andrew/Zotero/storage/MJZD7ADE/Liang et al. - 2023 - Can large language models provide useful feedback on research papers A large-scale empirical analys.pdf}
}

@article{liCanLargeLanguage2023,
  title = {Can Large Language Models Write Reflectively},
  author = {Li, Yuheng and Sha, Lele and Yan, Lixiang and Lin, Jionghao and Raković, Mladen and Galbraith, Kirsten and Lyons, Kayley and Gašević, Dragan and Chen, Guanliang},
  date = {2023-01-01},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {4},
  pages = {100140},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2023.100140},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X2300019X},
  urldate = {2024-11-11},
  abstract = {Generative Large Language Models (LLMs) demonstrate impressive results in different writing tasks and have already attracted much attention from researchers and practitioners. However, there is limited research to investigate the capability of generative LLMs for reflective writing. To this end, in the present study, we have extensively reviewed the existing literature and selected 9 representative prompting strategies for ChatGPT – the chatbot based on state-of-art generative LLMs to generate a diverse set of reflective responses, which are combined with student-written reflections. Next, those responses were evaluated by experienced teaching staff following a theory-aligned assessment rubric that was designed to evaluate student-generated reflections in several university-level pharmacy courses. Furthermore, we explored the extent to which Deep Learning classification methods can be utilised to automatically differentiate between reflective responses written by students vs. reflective responses generated by ChatGPT. To this end, we harnessed BERT, a state-of-art Deep Learning classifier, and compared the performance of this classifier to the performance of human evaluators and the AI content detector by OpenAI. Following our extensive experimentation, we found that (i) ChatGPT may be capable of generating high-quality reflective responses in writing assignments administered across different pharmacy courses, (ii) the quality of automatically generated reflective responses was higher in all six assessment criteria than the quality of student-written reflections; and (iii) a domain-specific BERT-based classifier could effectively differentiate between student-written and ChatGPT-generated reflections, greatly surpassing (up to 38\% higher across four accuracy metrics) the classification performed by experienced teaching staff and general-domain classifier, even in cases where the testing prompts were not known at the time of model training.},
  keywords = {ChatGPT,Generative language model,Natural language processing,Reflective writing},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-11T08:00:17.103Z},
  file = {/Users/andrew/Zotero/storage/L9SMTW53/S2666920X2300019X.html}
}

@article{munafoManifestoReproducibleScience2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  date = {2017-01-10},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {1},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  url = {https://www.nature.com/articles/s41562-016-0021},
  urldate = {2024-11-10},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  langid = {english},
  keywords = {Social sciences},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-10T14:59:06.865Z},
  file = {/Users/andrew/Zotero/storage/77Q5JRKX/Munafò et al. - 2017 - A manifesto for reproducible science.pdf}
}

@online{schulhoffPromptReportSystematic2024,
  title = {The {{Prompt Report}}: {{A Systematic Survey}} of {{Prompting Techniques}}},
  shorttitle = {The {{Prompt Report}}},
  author = {Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and Dulepet, Pranav Sandeep and Vidyadhara, Saurav and Ki, Dayeon and Agrawal, Sweta and Pham, Chau and Kroiz, Gerson and Li, Feileen and Tao, Hudson and Srivastava, Ashay and Da Costa, Hevander and Gupta, Saloni and Rogers, Megan L. and Goncearenco, Inna and Sarli, Giuseppe and Galynker, Igor and Peskoff, Denis and Carpuat, Marine and White, Jules and Anadkat, Shyamal and Hoyle, Alexander and Resnik, Philip},
  date = {2024-06-06},
  eprint = {2406.06608},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2406.06608},
  urldate = {2024-06-14},
  abstract = {Generative Artificial Intelligence (GenAI) systems are being increasingly deployed across all parts of industry and research settings. Developers and end users interact with these systems through the use of prompting or prompt engineering. While prompting is a widespread and highly researched concept, there exists conflicting terminology and a poor ontological understanding of what constitutes a prompt due to the area's nascency. This paper establishes a structured understanding of prompts, by assembling a taxonomy of prompting techniques and analyzing their use. We present a comprehensive vocabulary of 33 vocabulary terms, a taxonomy of 58 text-only prompting techniques, and 40 techniques for other modalities. We further present a meta-analysis of the entire literature on natural language prefix-prompting.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/andrew/Zotero/storage/HUITTKA7/Schulhoff et al_2024_The Prompt Report.pdf;/Users/andrew/Zotero/storage/FMR4FLJQ/2406.html}
}

@article{shanahanRolePlayLarge2023,
  title = {Role Play with Large Language Models},
  author = {Shanahan, Murray and McDonell, Kyle and Reynolds, Laria},
  date = {2023-11-08},
  journaltitle = {Nature},
  pages = {1--6},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06647-8},
  url = {https://www.nature.com/articles/s41586-023-06647-8},
  urldate = {2023-11-08},
  abstract = {As dialogue agents become increasingly human-like in their performance, we must develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. Here we foreground the concept of role play. Casting dialogue-agent behaviour in terms of role play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models that they in fact lack. Two important cases of dialogue-agent behaviour are addressed this way, namely, (apparent) deception and (apparent) self-awareness.},
  langid = {english},
  keywords = {Computer science,Philosophy},
  file = {/Users/andrew/Zotero/storage/XVYR9QDK/Shanahan et al_2023_Role play with large language models.pdf}
}

@online{sharmaUnderstandingSycophancyLanguage2023,
  title = {Towards {{Understanding Sycophancy}} in {{Language Models}}},
  author = {Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R. and Cheng, Newton and Durmus, Esin and Hatfield-Dodds, Zac and Johnston, Scott R. and Kravec, Shauna and Maxwell, Timothy and McCandlish, Sam and Ndousse, Kamal and Rausch, Oliver and Schiefer, Nicholas and Yan, Da and Zhang, Miranda and Perez, Ethan},
  date = {2023-10-27},
  eprint = {2310.13548},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2310.13548},
  url = {http://arxiv.org/abs/2310.13548},
  urldate = {2024-09-08},
  abstract = {Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.6,Statistics - Machine Learning},
  file = {/Users/andrew/Zotero/storage/W3PV4TMS/Sharma et al. - 2023 - Towards Understanding Sycophancy in Language Models.pdf;/Users/andrew/Zotero/storage/DIYLTKRG/2310.html}
}

@online{spilliasEvaluatingGenerativeAI2024,
  title = {Evaluating {{Generative AI}} to {{Extract Qualitative Data}} from {{Peer-Reviewed Documents}}},
  author = {Spillias, Scott and Ollerhead, Katherine and Andreotta, Matthew and Annand-Jones, Ruby and Boschetti, Fabio and Duggan, Joseph and Karcher, Denis and Paris, Cecile and Shellock, Rebecca and Trebilco, Rowan},
  date = {2024-08-26},
  eprinttype = {Research Square},
  issn = {2693-5015},
  doi = {10.21203/rs.3.rs-4922498/v1},
  url = {https://www.researchsquare.com/article/rs-4922498/v1},
  urldate = {2024-11-08},
  abstract = {Uptake of AI tools in knowledge production processes is rapidly growing. Here, we explore the ability of generative AI tools to reliably extract qualitative data from peer-reviewed documents. Specifically, we evaluate the capacity of multiple AI tools to analyse literature and extract relevant information for a systematic literature review, comparing the results to those of human reviewers. We address how well AI tools can discern the presence of relevant contextual data, whether the outputs of AI tools are comparable to human extractions, and whether the difficulty of question influences the performance of the extraction. While the AI tools we tested (GPT4-Turbo and Elicit) were not reliable in discerning the presence or absence of contextual data, at least one of the AI tools consistently returned responses that were on par with human reviewers. These results highlight the utility of AI tools in the extraction phase of evidence synthesis for supporting human-led reviews and underscore the ongoing need for human oversight.},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-08T19:49:24.448Z},
  file = {/Users/andrew/Zotero/storage/2UJPEMEC/Spillias et al. - 2024 - Evaluating Generative AI to Extract Qualitative Data from Peer-Reviewed Documents.pdf}
}

@online{stechlyChainThoughtlessnessAnalysis2024,
  title = {Chain of {{Thoughtlessness}}? {{An Analysis}} of {{CoT}} in {{Planning}}},
  shorttitle = {Chain of {{Thoughtlessness}}?},
  author = {Stechly, Kaya and Valmeekam, Karthik and Kambhampati, Subbarao},
  date = {2024-05-08},
  url = {https://arxiv.org/abs/2405.04776v2},
  urldate = {2024-10-02},
  abstract = {Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting-a method of demonstrating solution procedures-with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem. This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples. We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes. Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.},
  langid = {english},
  organization = {arXiv.org},
  file = {/Users/andrew/Zotero/storage/K373ZN9U/Stechly et al. - 2024 - Chain of Thoughtlessness An Analysis of CoT in Planning.pdf}
}

@article{toner-rodgersArtificialIntelligenceScientific,
  title = {Artificial {{Intelligence}}, {{Scientific Discovery}}, and {{Product Innovation}}},
  author = {Toner-Rodgers, Aidan},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-09T21:09:14.350Z},
  file = {/Users/andrew/Zotero/storage/SZ2BM2CS/Toner-Rodgers - Artificial Intelligence, Scientific Discovery, and Product Innovation.pdf}
}

@online{valmeekamPlanningAbilitiesLarge2023,
  title = {On the {{Planning Abilities}} of {{Large Language Models}} : {{A Critical Investigation}}},
  shorttitle = {On the {{Planning Abilities}} of {{Large Language Models}}},
  author = {Valmeekam, Karthik and Marquez, Matthew and Sreedharan, Sarath and Kambhampati, Subbarao},
  date = {2023-11-06},
  eprint = {2305.15771},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.15771},
  url = {http://arxiv.org/abs/2305.15771},
  urldate = {2024-08-12},
  abstract = {Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs in LLM-Modulo settings where they act as a source of heuristic guidance for external planners and verifiers. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of \textasciitilde 12\% across the domains. However, the results in the LLM-Modulo setting show more promise. In the LLM-Modulo setting, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.},
  pubstate = {prepublished},
  keywords = {/unread,Computer Science - Artificial Intelligence},
  file = {/Users/andrew/Zotero/storage/TGX5Z6G7/Valmeekam et al. - 2023 - On the Planning Abilities of Large Language Models  A Critical Investigation.pdf;/Users/andrew/Zotero/storage/MNK8QW5L/2305.html}
}

@article{yanaiItTakesTwo2024,
  title = {It Takes Two to Think},
  author = {Yanai, Itai and Lercher, Martin J.},
  date = {2024-01},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotechnol},
  volume = {42},
  number = {1},
  pages = {18--19},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/s41587-023-02074-2},
  url = {https://www.nature.com/articles/s41587-023-02074-2},
  urldate = {2024-10-16},
  langid = {english},
  keywords = {Education,Lab life},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-10-16T20:30:14.063Z},
  file = {/Users/andrew/Zotero/storage/7M7Y5L8T/Screenshot 2024-10-16 at 22.30.57.png}
}

@online{yanPromisesChallengesGenerative2024b,
  title = {Promises and Challenges of Generative Artificial Intelligence for Human Learning},
  author = {Yan, Lixiang and Greiff, Samuel and Teuber, Ziwen and Gašević, Dragan},
  date = {2024-09-05},
  eprint = {2408.12143},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2408.12143},
  url = {http://arxiv.org/abs/2408.12143},
  urldate = {2024-11-11},
  abstract = {Generative artificial intelligence (GenAI) holds the potential to transform the delivery, cultivation, and evaluation of human learning. This Perspective examines the integration of GenAI as a tool for human learning, addressing its promises and challenges from a holistic viewpoint that integrates insights from learning sciences, educational technology, and human-computer interaction. GenAI promises to enhance learning experiences by scaling personalised support, diversifying learning materials, enabling timely feedback, and innovating assessment methods. However, it also presents critical issues such as model imperfections, ethical dilemmas, and the disruption of traditional assessments. Cultivating AI literacy and adaptive skills is imperative for facilitating informed engagement with GenAI technologies. Rigorous research across learning contexts is essential to evaluate GenAI's impact on human cognition, metacognition, and creativity. Humanity must learn with and about GenAI, ensuring it becomes a powerful ally in the pursuit of knowledge and innovation, rather than a crutch that undermines our intellectual abilities.},
  pubstate = {prepublished},
  keywords = {Computer Science - Human-Computer Interaction},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-11T07:54:48.733Z},
  file = {/Users/andrew/Zotero/storage/9IP6TBAD/Yan et al. - 2024 - Promises and challenges of generative artificial intelligence for human learning.pdf;/Users/andrew/Zotero/storage/GV9WA4BS/2408.html}
}

@inproceedings{zamfirescu-pereiraWhyJohnnyCant2023,
  title = {Why {{Johnny Can}}’t {{Prompt}}: {{How Non-AI Experts Try}} (and {{Fail}}) to {{Design LLM Prompts}}},
  shorttitle = {Why {{Johnny Can}}’t {{Prompt}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zamfirescu-Pereira, J.D. and Wong, Richmond Y. and Hartmann, Bjoern and Yang, Qian},
  date = {2023-04-19},
  pages = {1--21},
  publisher = {ACM},
  location = {Hamburg Germany},
  doi = {10.1145/3544548.3581388},
  url = {https://dl.acm.org/doi/10.1145/3544548.3581388},
  urldate = {2024-07-29},
  abstract = {Pre-trained large language models (“LLMs”) like GPT-3 can engage in fuent, multi-turn instruction-taking out-of-the-box, making them attractive materials for designing natural language interactions. Using natural language to steer LLM outputs (“prompting”) has emerged as an important design technique potentially accessible to non-AI-experts. Crafting efective prompts can be challenging, however, and prompt-based interactions are brittle. Here, we explore whether non-AI-experts can successfully engage in “end-user prompt engineering” using a design probe—a prototype LLM-based chatbot design tool supporting development and systematic evaluation of prompting strategies. Ultimately, our probe participants explored prompt designs opportunistically, not systematically, and struggled in ways echoing end-user programming systems and interactive machine learning systems. Expectations stemming from human-to-human instructional experiences, and a tendency to overgeneralize, were barriers to efective prompt design. These fndings have implications for non-AI-expert-facing LLM-based tool design and for improving LLM-and-prompt literacy among programmers and the public, and present opportunities for further research.},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9421-5},
  langid = {english},
  file = {/Users/andrew/Zotero/storage/ZKAIA4AR/Zamfirescu-Pereira et al. - 2023 - Why Johnny Can’t Prompt How Non-AI Experts Try (and Fail) to Design LLM Prompts.pdf}
}
